# TutoLLM
Pour comprendre, depuis la base (avec de simples additions et multiplications), comment est construit chatGPT à partir de la brique fondamentale : le perceptron, en passant par le transformer. 
[eBook gratuit : comprendre ChatGPT](https://boutique.yoga-pro.xyz/b/FW3oG)

3 démos pour "jouer" avec les fondements des réseaux de neuronnes pour se faire une intuition de ce qu'il y a dans chatGPT
1. Le [perceptron](https://johannoriel.github.io/TutoLLM/perceptron.html) : l'exemple le plus minimaliste qui soit, mais, tout est déjà là
2. Le [MLP](https://johannoriel.github.io/TutoLLM/learner.html), quand on met des perceptrons en réseau, et qu'on leur enseigne des choses avec la "rétropropagation"
3. Ici c'est un [simulateur](https://johannoriel.github.io/TutoLLM/genetic.html) de "vers de terre" ou de "créature" qui doit apprendre à se déplacer et éviter des obstacles, mais cette fois ci, le réseau de neuronnes apprends avec un algorithme génétique.


Autres ressources utiles :
* [Number recognizer](https://machinelearning.tobiashill.se/2019/01/28/extra-2-a-mnist-playground/)
* [Incredible LLM visualizer](https://bbycroft.net/llm)
* [Inside an ANN](https://x.com/gabeElbling/status/1850220333631943068)
* [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
